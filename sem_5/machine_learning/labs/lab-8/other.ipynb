{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AUCROCSGDOptimizer:\n",
    "    def __init__(self, learning_rate=0.01, batch_size=32, n_iter=1000, seed=None):\n",
    "        \"\"\"\n",
    "        Optimizer for AUC ROC using SGD.\n",
    "\n",
    "        :param learning_rate: Learning rate for SGD.\n",
    "        :param batch_size: Number of samples in each batch for generating pairs.\n",
    "        :param n_iter: Number of iterations for training.\n",
    "        :param seed: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.seed = seed\n",
    "        self.weights = None\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def _generate_pairs(self, X, y):\n",
    "        \"\"\"\n",
    "        Generate positive and negative pairs for training.\n",
    "\n",
    "        :param X: Input feature matrix of shape (n_samples, n_features).\n",
    "        :param y: Target labels of shape (n_samples,).\n",
    "        :return: Two sets of indices for positive and negative samples.\n",
    "        \"\"\"\n",
    "        positive_indices = np.where(y == 1)[0]\n",
    "        negative_indices = np.where(y == 0)[0]\n",
    "\n",
    "        pos_batch = self.rng.choice(positive_indices, self.batch_size, replace=True)\n",
    "        neg_batch = self.rng.choice(negative_indices, self.batch_size, replace=True)\n",
    "\n",
    "        return pos_batch, neg_batch\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using SGD to optimize AUC ROC.\n",
    "\n",
    "        :param X: Input feature matrix of shape (n_samples, n_features).\n",
    "        :param y: Target labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        self.weights = np.zeros(n_features)  # Initialize weights\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            pos_batch, neg_batch = self._generate_pairs(X, y)\n",
    "\n",
    "            X_pos = X[pos_batch]\n",
    "            X_neg = X[neg_batch]\n",
    "\n",
    "            # Pairwise differences\n",
    "            margin = X_pos - X_neg\n",
    "            logits = margin @ self.weights\n",
    "\n",
    "            # Compute gradient using sigmoid\n",
    "            sigmoid_grad = 1 / (1 + np.exp(logits))\n",
    "            gradient = -np.mean(margin.T * sigmoid_grad, axis=1)\n",
    "\n",
    "            # Update weights\n",
    "            self.weights -= self.learning_rate * gradient\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probabilities for positive class.\n",
    "\n",
    "        :param X: Input feature matrix of shape (n_samples, n_features).\n",
    "        :return: Probabilities of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        logits = X @ self.weights\n",
    "        return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict binary class labels.\n",
    "\n",
    "        :param X: Input feature matrix of shape (n_samples, n_features).\n",
    "        :return: Predicted class labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import RegressorMixin\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class AUCLogisticOptimizer(RegressorMixin):\n",
    "    def __init__(self, lr=0.01, momentum=1, delta_converged=1e-3, max_steps=1000, batch_size=64):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.delta_converged = delta_converged\n",
    "        self.max_steps = max_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.W = None  # Weight vector\n",
    "        self.velocity = None  # For momentum\n",
    "\n",
    "    def calculate_gradient(self, X1, Y1, X2, Y2):\n",
    "        logits_diff = (X1 @ self.W) - (X2 @ self.W)\n",
    "        sigmoid_values = sigmoid(-logits_diff)\n",
    "        grad_W = -np.mean((sigmoid_values[:, np.newaxis]) * (X1 - X2), axis=0)\n",
    "        return grad_W\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit the model to the data.\n",
    "        X: Features (n_samples, n_features).\n",
    "        Y: Labels (n_samples,).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.W = np.zeros(n_features)\n",
    "        self.velocity = np.zeros(n_features)\n",
    "\n",
    "        # Separate positive and negative classes\n",
    "        pos_indices = np.where(Y == 1)[0]\n",
    "        neg_indices = np.where(Y == 0)[0]\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            # Randomly sample positive and negative batches\n",
    "            pos_batch = np.random.choice(pos_indices, size=self.batch_size, replace=True)\n",
    "            neg_batch = np.random.choice(neg_indices, size=self.batch_size, replace=True)\n",
    "\n",
    "            X1, Y1 = X[pos_batch], Y[pos_batch]\n",
    "            X2, Y2 = X[neg_batch], Y[neg_batch]\n",
    "\n",
    "            # Calculate gradient\n",
    "            grad_W = self.calculate_gradient(X1, Y1, X2, Y2)\n",
    "\n",
    "            # Apply momentum and update weights\n",
    "            self.velocity = self.momentum * self.velocity - self.lr * grad_W\n",
    "            self.W += self.velocity\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(self.lr * grad_W) < self.delta_converged:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = sigmoid(X @ self.W)\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probabilities = sigmoid(X @ self.W)\n",
    "        return np.column_stack([1 - probabilities, probabilities])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
