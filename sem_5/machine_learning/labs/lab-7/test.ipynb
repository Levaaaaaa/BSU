{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import inspect\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import typing as tp\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics._scorer import _check_multimetric_scoring\n",
    "from sklearn.model_selection._validation import _score\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "class MyAdaBoostClassifier:\n",
    "    \"\"\"\n",
    "    Multiclass AdaBoost implementation with SAMME.R algorithm\n",
    "    \"\"\"\n",
    "    big_number = 1 << 32\n",
    "    eps = 1e-8\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_estimators: int,\n",
    "            base_estimator: tp.Type[sklearn.base.BaseEstimator],\n",
    "            seed: int,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param n_estimators: count of estimators\n",
    "        :param base_estimator: base estimator (practically tree classifier)\n",
    "        :param seed: global seed\n",
    "        :param kwargs: keyword arguments of base estimator\n",
    "        \"\"\"\n",
    "        self.n_classes = None\n",
    "        self.error_history = []  # this is to track model learning process\n",
    "        self.n_estimators = n_estimators\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.base_estimator = base_estimator\n",
    "        self.base_estimator_kwargs = kwargs\n",
    "        # deduce which keywords are used to set seed for an estimator (sklearn or own tree implementation)\n",
    "        signature = inspect.signature(self.base_estimator.__init__)\n",
    "        self.seed_keyword = None\n",
    "        if 'seed' in signature.parameters:\n",
    "            self.seed_keyword = 'seed'\n",
    "        elif 'random_state' in signature.parameters:\n",
    "            self.seed_keyword = 'random_state'\n",
    "        self.estimators = []\n",
    "\n",
    "    def create_new_estimator(\n",
    "            self,\n",
    "            seed: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        create new base estiamtor with proper keywords\n",
    "        and new *unique* seed\n",
    "        :param seed:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        params = self.base_estimator_kwargs.copy()\n",
    "        if self.seed_keyword:\n",
    "            params[self.seed_keyword] = seed\n",
    "#        new_estimator = DecisionTreeClassifier(random_state=seed) #self.base_estimator(**self.base_estimator_kwargs)\n",
    "        \n",
    "        return self.base_estimator(**params)\n",
    "\n",
    "    def get_new_weights(self, \n",
    "                        true_labels: np.ndarray, \n",
    "                        predictions: np.ndarray, \n",
    "                        weights: np.ndarray):\n",
    "        \"\"\"\n",
    "        Обновляет веса объектов на основе алгоритма SAMME.R.\n",
    "        \n",
    "        Parameters:\n",
    "            true_labels (np.ndarray): Истинные метки классов (размер n).\n",
    "            predict_proba (np.ndarray): Предсказанные вероятности классов (размер n x n_classes).\n",
    "            weights (np.ndarray): Текущие веса объектов (размер n).\n",
    "            n_classes (int): Общее количество классов.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Новый массив весов (размер n).\n",
    "        \"\"\"\n",
    "        n_classes = self.n_classes\n",
    "        # Масштабирование вероятностей (логарифм и нормализация по числу классов)\n",
    "        eps = self.eps  # Чтобы избежать деления на ноль\n",
    "        probabilities = np.clip(predictions, eps, 1 - eps)  # Убираем нули и единицы\n",
    "        log_proba = np.log(probabilities)\n",
    "        \n",
    "        # Вычисляем значения обновления весов\n",
    "        # Индекс правильного класса для каждого объекта\n",
    "        true_class_log_proba = log_proba[np.arange(len(true_labels)), true_labels]\n",
    "        # Отрицательная \"ошибка\"\n",
    "        weight_update = -true_class_log_proba + (1 / n_classes) * log_proba.sum(axis=1)\n",
    "        \n",
    "        # Обновляем веса\n",
    "        new_weights = weights * np.exp(weight_update)\n",
    "        \n",
    "        # Нормируем веса\n",
    "        new_weights /= np.sum(new_weights)\n",
    "        \n",
    "        return new_weights\n",
    "\n",
    "    @staticmethod\n",
    "    def get_estimator_error(\n",
    "            estimator: sklearn.base.BaseEstimator,\n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            weights: np.ndarray\n",
    "    ):\n",
    "     #-> float:\n",
    "        \"\"\"\n",
    "        Вычисляет взвешенную ошибку эстиматора на выборке с весами.\n",
    "\n",
    "        Parameters:\n",
    "            estimator (BaseEstimator): Обученный оценщик (например, DecisionTreeClassifier).\n",
    "            X (np.ndarray): Матрица признаков (размер n x m).\n",
    "            y (np.ndarray): Вектор истинных меток классов (размер n).\n",
    "            weights (np.ndarray): Вектор весов объектов (размер n).\n",
    "\n",
    "        Returns:\n",
    "            float: Взвешенная ошибка эстиматора.\n",
    "        \"\"\"\n",
    "\n",
    "        # Предсказания модели\n",
    "        predictions = estimator.predict(X)\n",
    "        \n",
    "        # Индикатор неправильной классификации (1 для ошибок, 0 для правильных)\n",
    "        misclassified = (predictions != y).astype(float)\n",
    "        \n",
    "        # Взвешенная ошибка\n",
    "#        weighted_error = np.sum(weights * misclassified) / np.sum(weights)\n",
    "        predict_proba = np.clip(estimator.predict_proba(X), MyAdaBoostClassifier.eps, 1 - MyAdaBoostClassifier.eps)\n",
    "        weighted_error = np.sum(weights * (np.argmax(predict_proba, axis=1) != y))        \n",
    "        return weighted_error\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Sequentially fit estimators with updated weights on each iteration using SAMME.R algorithm.\n",
    "        \n",
    "        :param X: [n_samples, n_features]\n",
    "        :param y: [n_samples]\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.error_history = []\n",
    "\n",
    "        # Compute the number of classes for internal use\n",
    "        self.n_classes = len(np.unique(y, return_counts=False))\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Initialize weights uniformly over all samples\n",
    "        weights = np.full((n_samples,), fill_value=1 / n_samples)\n",
    "\n",
    "        # Sequentially fit each model and adjust weights\n",
    "        for seed in self.rng.choice(\n",
    "                max(MyAdaBoostClassifier.big_number, self.n_estimators),\n",
    "                size=self.n_estimators,\n",
    "                replace=False\n",
    "        ):\n",
    "            # Generate a unique seed for reproducibility\n",
    "          #  seed = self.rng.integers(0, 1e6)\n",
    "\n",
    "            # Add newly created estimator\n",
    "            estimator = self.create_new_estimator(seed)\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "            # Fit the estimator to data with current sample weights\n",
    "            estimator.fit(X=X, y=y, sample_weight=weights)\n",
    "\n",
    "            # Get predicted probabilities for SAMME.R\n",
    "            predict_proba = np.clip(estimator.predict_proba(X), self.eps, 1 - self.eps)\n",
    "\n",
    "            # Compute estimator error (misclassification rate weighted by sample weights)\n",
    "            estimator_error = self.get_estimator_error(estimator, X, y, weights)\n",
    "\n",
    "            if estimator_error >= 1 - self.eps:\n",
    "                estimator_error = 1 - self.eps  # Avoid division by zero or negative errors\n",
    "            if estimator_error == 0:\n",
    "                estimator_error = MyAdaBoostClassifier.eps\n",
    "\n",
    "            # Calculate alpha (model weight) for SAMME.R\n",
    "            estimator_alpha = 0.5 * np.log((1 - estimator_error) / estimator_error)\n",
    "\n",
    "                # Update sample weights using SAMME.R formula\n",
    "            true_class_log_proba = np.log(predict_proba[np.arange(n_samples), y])\n",
    "            weights_update_factor = -true_class_log_proba + (1 / self.n_classes) * np.log(predict_proba).sum(axis=1)\n",
    "            weights *= np.exp(estimator_alpha * weights_update_factor)\n",
    "\n",
    "            # Normalize weights\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "            # Append the error to the error history\n",
    "            self.error_history.append(estimator_error)\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Новый базовый оценщик: DecisionTreeClassifier(random_state=42)\n",
      "Параметры нового оценщика: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10024\\1212814932.py:186: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  estimator_alpha = 0.5 * np.log((1 - estimator_error) / estimator_error)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10024\\1212814932.py:194: RuntimeWarning: invalid value encountered in divide\n",
      "  weights /= np.sum(weights)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MyAdaBoostClassifier at 0x19689db9210>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Инициализация AdaBoostClassifier\n",
    "boost = MyAdaBoostClassifier(seed=42, n_estimators=1, base_estimator=DecisionTreeClassifier)\n",
    "\n",
    "# Создаём новый базовый оценщик\n",
    "new_estimator = boost.create_new_estimator(seed=42)\n",
    "\n",
    "# Проверяем параметры нового оценщика\n",
    "print(\"Новый базовый оценщик:\", new_estimator)\n",
    "print(\"Параметры нового оценщика:\", new_estimator.get_params())\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "\n",
    "boost.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
